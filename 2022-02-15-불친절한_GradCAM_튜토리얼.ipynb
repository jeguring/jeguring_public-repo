{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4960506a",
   "metadata": {},
   "source": [
    "# 불친절한 튜토리얼 - Grad-CAM을 이용하는 법\n",
    "친구가 파이썬, 딥러닝도 모르면서 Grad CAM은 써보고 싶다고 한다. 인중을 때려주고 싶지만, 소중한 뉴비가 이 분야를 이탈하지 않도록 도와주기 위해 이 코드를 작성한다.  \n",
    "\n",
    "먼저 나중에 사용할 값들을 변수에 저장해놓자. 다 피가 되고 살이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073f044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa317f",
   "metadata": {},
   "source": [
    "필요한 라이브러리들을 불러오자. 오류나면 설치하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "445c5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b0da9",
   "metadata": {},
   "source": [
    "## MNIST 데이터셋 불러오기\n",
    "- 케라스 내장 함수를 사용하여 MNIST 데이터를 불러온다. 실전에서는 당신의 데이터를 불러오면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b95d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c8f8d",
   "metadata": {},
   "source": [
    "- 데이터가 몇 개나 있나 확인해보자\n",
    "    - 훈련 데이터는 60,000개, 테스트 데이터는 10,000개가 있으며 각 데이터는 $28 \\times 28$ 행렬에 0부터 255 사이의 픽셀값이 적혀 있는 사진이다. 클래스 값으로는 0부터 9까지 가질 수 있다.\n",
    "- `np.ndarray`의 `shape` attribute는 변수의 크기를 나타낸다. \n",
    "    - attribute란? 파이썬에는 **객체**라는 것이 존재한다. 객체 안에는 데이터도 넣을 수 있고, 함수도 넣을 수 있고, 이것 저것 다 넣을 수 있다. 넘파이 배열 (`np.ndarray`)은 객체다. attribute는 객체 안에 저장되어 있는 데이터 중 하나다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3b38c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac52cb",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "- CNN은 4차원 텐서 (데이터 개수, 높이, 너비, 채널)를 입력 받는다. 현재 `X_train`은 채널을 위한 축이 없기 때문에 이를 추가해줘야 한다.\n",
    "- 딥러닝 프레임워크는 32비트 `float` 자료형을 입력 받는다.\n",
    "- 딥러닝 프레임워크는 보통 데이터를 -1과 1사이 값으로 만들어준다.\n",
    "- 클래스는 원핫인코딩이라는 것을 해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2975c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[..., np.newaxis]\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train = X_train / 255.0\n",
    "\n",
    "X_test = X_test[..., np.newaxis]\n",
    "X_test = X_test.astype(np.float32)\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb89a8a",
   "metadata": {},
   "source": [
    "## 간단한 CNN 모델 만들기\n",
    "- CNN 모델을 만든다. CNN 먼저 공부하고 오기를 바란다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ee0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_cnn(input_shape=(28, 28, 1), num_classes=10):\n",
    "    cnn = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=input_shape),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(num_classes),\n",
    "        keras.layers.Softmax(),\n",
    "    ])\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162fbb7",
   "metadata": {},
   "source": [
    "- 만든 모델은 대충 아래 표처럼 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4acc608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                31370     \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,186\n",
      "Trainable params: 50,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 19:34:24.158477: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-02-15 19:34:24.158629: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = get_basic_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6418da3",
   "metadata": {},
   "source": [
    "## 모델의 손실 함수, 최적화 알고리즘, 평가지표 설정\n",
    "\n",
    "### 손실 함수란?\n",
    "- 딥러닝 모델은 복잡해보이지만 그냥 하나의 함수라고 이해하면 된다. $f(x)=ax^2+bx+c$와 같은 2차 함수를 예로 들어보자. 입력값 $x$ 하나가 들어가면 출력값 $f(x)$가 나온다. $f(x)$는 $ax^2+bx+c$의 연산을 한다. 이 연산의 결과는 $a, b, c$ 값에 따라 달라진다. 딥러닝은 매우 복잡하게 생긴 함수로 이해하자.\n",
    "- 위의 예시처럼 딥러닝 모델에는 $a, b, c$ 같은 학습 가능한 파라미터가 있다. 학습한다는 것은 $a, b, c$ 값을 점점 좋은 값으로 설정해준다는 것이다. 우리가 사용할 수 있는 $a, b, c$는 무수히 많을텐데, 좋은 $a, b, c$란 무엇일까?\n",
    "- 좋은 $a, b, c$란 **손실 함수**를 최소로 만들어주는 $a, b, c$이다. 데이터를 $x$, 대응하는 정답을 $y$라고 해보자. 우리의 모델의 예측값은 $f(x)$라고 말했다. 그럼 $(y-f(x))$는 무엇이 되는가? 바로 오차가 된다. 우리의 바람은 $f(x)$가 $y$와 최대한 비슷해지는 것이다. 우리는 무수히 많은 $a, b, c$ 선택지 중에서 오차를 가장 작게 만들어주는 $a, b, c$를 찾을 것이다.\n",
    "- 설명은 하나의 데이터에서 발생한 오차만을 고려했지만, 손실 함수는 보통 모든 데이터의 평균 오차를 최소화하게 된다. 오차는 양수일 수도 있고 음수일 수도 있기 때문에 모든 데이터의 오차를 더하면 양수와 음수가 서로 상쇄될 수 있다. 이를 방지하기 위해 오차 제곱의 평균을 최소화해준다. $\\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2$\n",
    "- 분류 문제에서는 위 손실 함수보다 **Cross Entropy**라는 손실 함수를 사용한다. 이건 당신이 공부하길 바란다. 하지만 철학을 동일하다. 모델의 예측 값 $f(x)$와 실제 클래스 $y$의 차이를 다 평균낸 것이다.\n",
    "\n",
    "### 최적화 알고리즘이란?\n",
    "- 우리의 목표를 정리하자면 우리의 모델 $f(x)$에 들어있는 학습 가능한 파라미터를 손실 함수를 최소로 만들어주는 녀석들로 설정하고 싶다.\n",
    "- 학창 시절 함수의 최소값을 구하기 위해 함수를 미분하여 0되는 지점을 모두 조사했었다. 하지만 손실 함수는 굉장히 복잡해서 미분하여 0되는 지점을 계산할 수 없다.\n",
    "- 그래서 이용하는 것이 최적화 알고리즘이다. 다 됐고, 손실함수를 낮추는 방향으로 학습 가능한 파라미터 $a, b, c$를 조금씩 조정하는 알고리즘이다. 그냥 adam 쓰셈\n",
    "\n",
    "### 평가지표란?\n",
    "- 손실 함수는 오차들의 평균이라고 했다. 그 값을 사람이 보기엔 의미를 해석하기 어렵다. 평가 지표는 사람이 이해하기 쉬운 다른 좋은 지표이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19680fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e8a9b",
   "metadata": {},
   "source": [
    "## 모델 학습\n",
    "- 데이터를 직접 넣어서 $f(x)$를 구하고, 오차를 구하며, 모델 파라미터를 조정하는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb5188b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 19:34:24.274108: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-02-15 19:34:24.436238: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745/750 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 19:34:31.644133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750/750 [==============================] - 8s 11ms/step - loss: 0.2157 - acc: 0.9341 - val_loss: 0.0729 - val_acc: 0.9775\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0647 - acc: 0.9803 - val_loss: 0.0546 - val_acc: 0.9840\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0470 - acc: 0.9857 - val_loss: 0.0487 - val_acc: 0.9854\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0360 - acc: 0.9887 - val_loss: 0.0403 - val_acc: 0.9877\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0302 - acc: 0.9901 - val_loss: 0.0389 - val_acc: 0.9883\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 8s 10ms/step - loss: 0.0238 - acc: 0.9924 - val_loss: 0.0471 - val_acc: 0.9858\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0206 - acc: 0.9936 - val_loss: 0.0410 - val_acc: 0.9872\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 8s 10ms/step - loss: 0.0172 - acc: 0.9945 - val_loss: 0.0363 - val_acc: 0.9900\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0132 - acc: 0.9956 - val_loss: 0.0417 - val_acc: 0.9887\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 8s 11ms/step - loss: 0.0114 - acc: 0.9962 - val_loss: 0.0431 - val_acc: 0.9887\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599f753",
   "metadata": {},
   "source": [
    "## Grad-CAM\n",
    "여기서부터 **Grad CAM** 내용이다. **Grad CAM**은 CNN 모델의 가장 마지막 feature maps에 대해서 모델의 출력값을 미분하는 과정을 포함한다. \n",
    "\n",
    "\n",
    "### Feature maps이란\n",
    "화가 난다. feature maps을 설명해야 한다. 그것도 아주 쉽게.\n",
    "\n",
    "\n",
    "딥러닝은 흔히 여러층으로 구성되어 있다고 한다. 각 층에서 일어나는 것은 **데이터 변환**이다. 예를 들어, 우리의 데이터가 4차원 벡터 $(179, 70, 29, -7.5)^\\top$라고 하자. 4차원 벡터가 딥러닝에 들어가서 첫 번째 은닉층 (hidden layer)를 지나면 더 좋은 벡터가 된다. 예를 들어, 4차원 벡터가 10차원 벡터 $(-1.44,\\; 0.488, \\;\\cdots, \\;0.29)^\\top$. 이 10차원 벡터를 만드는 과정에 위에서 말한 학습가능한 파라미터가 있는 것이다. 좋은 파라미터는 좋은 10차원 벡터를 만들어낸다. 10차원 벡터는 층을 지나면 다른 벡터가 된다. 이렇게 더 좋은 벡터로 만들어주는 과정을 반복한다. 그리고 마지막 벡터의 원소들로 연산을 해줘서 마지막 예측 값을 내뱉게 된다. 이 모든 과장에 학습 가능한 파라미터가 있다. 손실 함수를 줄여주는 방향으로 파라미터가 점점 조정되면, 각 층에서는 점점 문제를 풀기 좋은 벡터를 만들어낼 수 있게 된다.\n",
    "\n",
    "CNN은 위의 설명에서 벡터를 이미지로 바꿔주면 된다. CNN은 이미지를 입력 받는다. 예를 들어, $28 \\times 28 \\times 3$ 크기의 이미지를 입력 받는다. 합성곱층 (convolutional layer)를 하나 지나면 더 좋은 이미지로 바뀌게 된다. 예를 들어, $26 \\times 26 \\times 32$ 크기의 이미지가 된다. 사실, 이 친구에게 이미지라는 단어는 더 이상 사용할 수 없을 것이다. 대신 우리는 이 친구를 feature map이라고 부른다. 즉, feature map은 합성곱층을 지나면서 만들어진 더 좋은 이미지라고 이해하면 쉽다.\n",
    "\n",
    "\n",
    "**Grad CAM**에서 모델의 마지막 feature maps을 사용한다는 것은 그냥 마지막 합성곱층을 지난 좋은 이미지로 이해하면 된다.\n",
    "\n",
    "---\n",
    "\n",
    "### 왜 미분하는데\n",
    "화가 난다. 왜 미분을 해야하는지 설명을 해야 한다. 그것도 아주 쉽게.\n",
    "\n",
    "미분이 무엇인가? $x$에서 함수 $f(x)$의 미분 값은 $x$가 아주 조금 변했을 때 $f(x)$가 얼마나 변하느냐를 나타내는 것이다.\n",
    "\n",
    "그럼 마지막 feature maps으로 우리의 모델을 미분한다는 것은 무슨 의미인가? feature maps이 아주 조금 변했을 때, 우리의 모델이 얼마나 변하느냐를 나타낸다. 모델이 많이 변하고 조금 변하는 것이 무엇을 의미할까? 개와 고양이를 분류하는 예시를 생각해보자. 지금 우리의 모델이 충분히 훌륭해서 개는 개라고, 고양이는 고양이라고 잘 분류한다고 가정하자. 이때, 개 사진에서 얼굴을 고양이로 바꿔보자. 모델의 예측값은 개에서 고양이로 급변할 것이다. 반면, 사진의 배경을 맑은 날에서 흐린 날로 바꾼다고 해서 모델의 예측 값이 개에서 고양이로 바뀌진 않을 것이다. 이 예시가 무엇을 의미하는가? 사진에서 중요한 부분일 수록 조금만 변형을 가해도 모델의 예측값에 큰 변화를 준다. 마지막 feature maps에서 중요한 영역은 조금만 변해도 모델의 예측값이 크게 변하기 때문에 큰 미분값을 가질 것이고, 반대로 그다지 중요하지 않은 영역은 미분값이 작을 것이다.\n",
    "\n",
    "---\n",
    "\n",
    "글 쓰다가 귀찮아져서 코드를 그대로 베껴왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67bde1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6314809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = make_gradcam_heatmap(X_test[0:1], model, 'max_pooling2d_1')\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf723782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAcABwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDw+Nd8ip6nFa17oUlrEkiEuGGTgdKzrOIy3kKD+JwK9zs/Ca2fh955R5hMecEdOK+qzDNKGXUHVqrmd0kvU46a9pi6dFy5VK/4HgzKVOCMUlW9SYG9mUDGHIqpXfLl3g7plvR2NzwxBFNqkBlcKokB5+te0eOfGVpoOkW1vask5lj2nac44r59WR4zlGKn2NPmuJp8ebK746bjnFeJjclo4rGUsVs4Xuv5r/5BUcqtN0qjvHp5fMLmXz7mSXpvYtUVFFeylYErKx//2Q==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def save_and_display_gradcam(img, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))\n",
    "\n",
    "\n",
    "img = X_test[1200]\n",
    "heatmap =  make_gradcam_heatmap(img[np.newaxis], model, 'max_pooling2d_1')\n",
    "save_and_display_gradcam(img, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
